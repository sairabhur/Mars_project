{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\n",
      "Requirement already satisfied: urllib3 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from selenium)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: splinter in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\n",
      "Requirement already satisfied: selenium>=3.14.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from splinter)\n",
      "Requirement already satisfied: urllib3 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from selenium>=3.14.0->splinter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install splinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News - Scrape the NASA Mars News Site and collect the latest News Title and Paragragh Text. Assign the text to variables that you can reference later.¶ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = {}                                                               # initializes empty dictionary\n",
    "paragraph_text = []                                                          # initializes empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://mars.nasa.gov/\"                                          # base URL for finding paragraph text\n",
    "nasa_url = \"https://mars.nasa.gov/news/\"                                     # URL for initial scrape\n",
    "response_1 = req.get(nasa_url)                                               # acquires first response from URL\n",
    "\n",
    "nasa_soup = bs(response_1.text, 'html.parser')                               # sends response to beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_div = nasa_soup.find(class_=\"slide\")                                    # finds class\n",
    "soup_news = soup_div.find_all('a')                                           # finds all anchors\n",
    "news_title = soup_news[1].get_text().strip()                                 # extracts and cleans title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_p = soup_div.find_all('a', href=True)                                   # finds paragraphs\n",
    "soup_p_url = soup_p[0]['href']                                               # gets paragraphs URL\n",
    "paragraph_url = base_url + soup_p_url                                        # concatenates URL for paragraph\n",
    "response_2 = req.get(paragraph_url)                                          # acquires second response from URL\n",
    "para_soup = bs(response_2.text, \"html.parser\")                               # sends response to beautiful soup\n",
    "ww_paragraphs = para_soup.find(class_='wysiwyg_content')                     # finds class\n",
    "paragraphs = ww_paragraphs.find_all('p')                                     # finds paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs:                                                 # iterates through paragraphs\n",
    "    clean_paragraph = paragraph.get_text().strip()                           # extracts and cleans pa\n",
    "    paragraph_text.append(clean_paragraph)                                   # appends to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"news_title\"] = news_title                                         # adds title to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"paragraph_text_1\"] = paragraph_text[0]                            # adds paragraph summary to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"paragraph_text_2\"] = paragraph_text[1]                            # adds paragraph detail to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_title': 'NASA Announces Landing Site for Mars 2020 Rover',\n",
       " 'paragraph_text_1': 'NASA has chosen Jezero Crater as the landing site for its upcoming Mars 2020 rover mission after a five-year search, during which details of more than 60 candidate locations on the Red Planet were scrutinized and debated by the mission team and the planetary science community.',\n",
       " 'paragraph_text_2': \"The rover mission is scheduled to launch in July 2020 as NASA's next step in exploration of the Red Planet. It will not only seek signs of ancient habitable conditions – and past microbial life – but the rover also will collect rock and soil samples and store them in a cache on the planet's surface. NASA and ESA (European Space Agency) are studying future mission concepts to retrieve the samples and return them to Earth, so this landing site sets the stage for the next decade of Mars exploration.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data                                                                    # displays dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images - Visit the url for JPL's Featured Space Image. Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "\n",
    "Developer note: One of the Gotchas! I ran into with this assignment is that the URL provided does NOT consistently provide images of Mars. In response to this, I resorted to a brute-force attack, in light of assignment time constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"C:\\Program Files (x86)\\Google\\chromedriver\\chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jpl_fullsize_url = 'https://photojournal.jpl.nasa.gov/'                 # defines base URL for fullsize images\n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"      # defines search URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(jpl_url)                                                       # visits search URL with automated browser\n",
    "jpl_html = browser.html                                                      # acquires response from URL\n",
    "jpl_soup = bs(jpl_html, 'html.parser')                                       # sends response to beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_list = []                                                     # initializes empty list\n",
    "\n",
    "for image in jpl_soup.find_all('div',class_=\"img\"):                          # extracts all images\n",
    "    featured_image_list.append(image.find('img').get('src'))                 # appends URL to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_image = featured_image_list[0]                                       # extracts first image found\n",
    "temp_list_1 = feature_image.split('-')                                       # splits on '-' (removes size limiters)\n",
    "temp_list_2 = temp_list_1[0].split('/')                                      # splits on '/' (parses out base filename)\n",
    "featured_image_url = jpl_fullsize_url + temp_list_2[-1] + '.jpg'             # concatenates fullsize image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://photojournal.jpl.nasa.gov/PIA22883.jpg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_image_url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather - Visit the Mars Weather twitter account and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather¶ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"C:\\Program Files (x86)\\Google\\chromedriver\\chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser = Browser('chrome', headless=False)                                  # defines browser\n",
    "tweet_url = 'https://twitter.com/marswxreport?lang=en'                       # defines search URL\n",
    "browser.visit(tweet_url)                                                     # visits search URL with automated browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_html = browser.html                                                    # acquires response from URL\n",
    "tweet_soup = bs(tweet_html, 'html.parser')                                   # sends response to beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_info_list = []                                                       # initializes empty list\n",
    "\n",
    "# extracts all tweets from soup\n",
    "for weather_info in tweet_soup.find_all('p',class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"):\n",
    "    weather_info_list.append(weather_info.text.strip())                      # appends cleaned tweet to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in reversed(weather_info_list):                                    # loops through list backwards\n",
    "    if value[:3]=='Sol':                                                     # isolates weather tweet\n",
    "        mars_weather = value                                                 # assigns to variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sol 2242 (2018-11-26), high -2C/28F, low -70C/-93F, pressure at 8.48 hPa, daylight 06:29-18:45'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_weather   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts - Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_url = 'https://space-facts.com/mars/'                                  # defines search URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_list = pd.read_html(facts_url)                                          # extracts data from URL using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_df = fact_list[0]                                                      # converts list to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>Equatorial Diameter:</td>\n",
      "      <td>6,792 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Polar Diameter:</td>\n",
      "      <td>6,752 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mass:</td>\n",
      "      <td>6.42 x 10^23 kg (10.7% Earth)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Moons:</td>\n",
      "      <td>2 (Phobos &amp; Deimos)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orbit Distance:</td>\n",
      "      <td>227,943,824 km (1.52 AU)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orbit Period:</td>\n",
      "      <td>687 days (1.9 years)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Surface Temperature:</td>\n",
      "      <td>-153 to 20 °C</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>First Record:</td>\n",
      "      <td>2nd millennium BC</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Recorded By:</td>\n",
      "      <td>Egyptian astronomers</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "facts_table = facts_df.to_html(header=False, index=False)                    # converts dataframe to html table\n",
    "print(facts_table)                                                           # displays html table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemisperes - Visit the USGS Astrogeology site to obtain high resolution images for each of Mars' hemispheres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"C:\\Program Files (x86)\\Google\\chromedriver\\chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser = Browser('chrome', headless=False)                                  # defines browser                                 \n",
    "\n",
    "# defines search URL\n",
    "usgs_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(usgs_url)                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_html = browser.html                                                     # acquires response from URL\n",
    "usgs_soup = bs(usgs_html, 'html.parser')                                     # sends response to beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = []                                                   # Creates empty list\n",
    "\n",
    "products = usgs_soup.find('div', class_='result-list')                       # finds products\n",
    "hemispheres = products.find_all('div', class_='item')                        # finds hemispheres\n",
    "\n",
    "for hemisphere in hemispheres:                                               # iterates through hemispheres\n",
    "    title = hemisphere.find('div', class_='description')\n",
    "    \n",
    "    title_text = title.a.text                                                # extracts cleaned title\n",
    "    title_text = title_text.replace(' Enhanced', '')\n",
    "    browser.click_link_by_partial_text(title_text)                           # (automated) click\n",
    "    \n",
    "    usgs_html = browser.html                                                 # acquires response from URL\n",
    "    usgs_soup = bs(usgs_html, 'html.parser')                                 # sends response to beautiful soup\n",
    "    \n",
    "    image = usgs_soup.find('div', class_='downloads').find('ul').find('li')  # extracts image url\n",
    "    img_url = image.a['href']\n",
    "    \n",
    "    hemisphere_image_urls.append({'title': title_text, 'img_url': img_url})  # adds dictionary to list  \n",
    "    \n",
    "    browser.click_link_by_partial_text('Back')                               # (automated) click back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg',\n",
       "  'title': 'Cerberus Hemisphere'},\n",
       " {'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg',\n",
       "  'title': 'Schiaparelli Hemisphere'},\n",
       " {'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg',\n",
       "  'title': 'Syrtis Major Hemisphere'},\n",
       " {'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg',\n",
       "  'title': 'Valles Marineris Hemisphere'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemisphere_image_urls                                                        # displays list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()                                                               # closes automated browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
